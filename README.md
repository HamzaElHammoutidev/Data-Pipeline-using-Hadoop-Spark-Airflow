# Data Pipeline using HDFS, Spark And Airflow   

# Building Steps:   
 - Loading Data from Hadoop Cluster   
 - Data Exploration   
 - Use Case Objective   
 - Data Processing and Cleansing  
 - Data Modeling   
 - Data Fitting into Model   
 - Data Loading   
 - Data Quality Checks   
  
# Steps :   
 
 - [x] Connecting to Hadoop using Spark and Loading Data from HDFS
 - [x] Data Sampling to 100 random rows for Data Exploration 
 - [ ] Defining Data Pipeline End Use Case 
 - [ ] Deduplication, Missing Values 
 - [ ] Conceptual Modeling using Star Schema : Fact Tables & Dimensions Tables
 - [ ] Selecting Only Needed Columns for our Model
 - [ ] Loading Data from Spark to our Model and write it into parquet format
 - [ ] Data Tables Counts Checks and Joins Checks on Fact Table
